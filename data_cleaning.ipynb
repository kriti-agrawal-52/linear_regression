{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb0fff-d257-401c-9fef-b4975933831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054f3f0-2ad9-429d-b1b4-9b03adffaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c956aa2-6310-4333-9c67-67a830b17872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HousingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a289ecc-379d-4340-9c89-e5618991ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view all columns of the database in the output window\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea832a7-3993-4bc2-bae0-4442f7518691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing which all features have missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a3f94-6c3f-450c-96f4-9e71ef797d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns with missing values are CRIM, ZN, INDUS, CHAS, AGE, and LSTAT, each with 20 missing values. The target variable MEDV does not have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790467d-1d76-49c3-88e6-3fe54907bcdb",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b49b7a-52ff-44e1-8c3f-410cc47a7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef820f-224f-4c6d-9179-e55345fe4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99982d98-2e33-4788-ba41-b363e171f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we drop rows which have NaN values?\n",
    "missing_row_count = df.isna().any(axis=1).sum() # Count of rows with at least one missing (NaN) value\n",
    "print(missing_row_count)\n",
    "#number of rows in the dataframe\n",
    "len(df)\n",
    "\n",
    "#since dropping 112 rows would make our dataset very small, we will impute missing values in the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac309893-ae87-4775-8a99-4e1399b79ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting all features with missing values on the y-axis against row index helps us:\n",
    "\n",
    "- Visually inspect the **shape and structure** of each feature (e.g., smooth, noisy, or plateaued).\n",
    "- Identify whether features exhibit **collinearity** or similar patterns â€” useful when deciding KNN-based imputation.\n",
    "- Detect features that are **too erratic or zigzagged**, where local imputation (like KNN or interpolation) may be unreliable.\n",
    "- Understand whether values change **gradually or abruptly** across the dataset, which can influence imputation strategy.\n",
    "\n",
    "This kind of visualization provides intuition for selecting the most appropriate imputation method: \n",
    "mean/median for noisy distributions, KNN for locally structured features, and possibly dropping rows/features that are unstable or uninformative.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Should we normalise before plotting?\n",
    "No\n",
    "When you're visually inspecting each feature individually, you usually want to preserve their original scale and units, because that reflects the real-world meaning (e.g., AGE is in %, TAX is per $10k).\n",
    "You don't want to flatten or distort shapes via scaling before assessing their structure.\n",
    "For plots like your value vs. index, it's good to see actual values, not rescaled ones.\n",
    "\"\"\"\n",
    "\n",
    "# Features with missing values\n",
    "missing_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'AGE', 'LSTAT']\n",
    "colors = cm.get_cmap('tab10', len(missing_features))  # Clean color palette\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for i, feature in enumerate(missing_features):\n",
    "    plt.plot(df.index, df[feature], label=feature, color=colors(i), linewidth=1.5)\n",
    "\n",
    "plt.title(\"Features with Missing Values\", fontsize=16)\n",
    "plt.xlabel(\"Row Index\", fontsize=12)\n",
    "plt.ylabel(\"Value\", fontsize=12)\n",
    "plt.legend(title=\"Feature\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04745dc5-0dde-498a-89f0-90c2274349fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exploring collinearity between AGE and LSTAT to assess whether one can be used to impute the other.\n",
    "1. However, since both features contain missing values, using one to impute the other may introduce circular logic.\n",
    "2. This is problematic because each depends on incomplete information, making such imputation unreliable without prior handling.\n",
    "\n",
    "What can be done instead:\n",
    "- Impute one with a simpler method first (e.g., median for AGE),\n",
    "- Then use it to impute the other (e.g., LSTAT via regression or KNN),\n",
    "- Or exclude rows where both are missing if thatâ€™s acceptable and rare.\n",
    "\"\"\"\n",
    "df[['AGE', 'LSTAT']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e30df2-b3eb-465c-af7f-53a748d6e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing CHAS\n",
    "df['CHAS'].value_counts()\n",
    "# since CHAS is a categorical feature which is heavily imbalanced towards 0, we can impute missing values with 0, as it reflects the mode. And avoids introducing rare class bias (imputing 1s by mistake).\n",
    "chas_mode = df['CHAS'].mode()[0]\n",
    "df['CHAS'].fillna(chas_mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c9c56-0e72-475a-84fc-d6b1e389648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zigzag / Noisy patterns\n",
    "\n",
    "Features like AGE and ZN jump rapidly, without smooth local structure.\n",
    "â†’ These are bad candidates for KNN or interpolation, because:\n",
    "\n",
    "Nearby rows (in index) don't behave similarly.\n",
    "No smoothness = KNN may pull in totally unrelated neighbors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5fa11-2769-4184-9ff7-043aab8b6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing for AGE \n",
    "print(\"Mean AGE:\", df['AGE'].mean())\n",
    "print(\"Median AGE:\", df['AGE'].median())\n",
    "df['AGE'].hist(bins=20)\n",
    "#since the histogram is so right-skewed, the mean would be quite skewed, so, we will use median to impute missing age values.\n",
    "df['AGE'].fillna(df['AGE'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6039f3-0d60-469e-a94b-5fcb55219ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing for ZN\n",
    "print(\"Mean ZN:\", df['ZN'].mean())\n",
    "print(\"Median ZN:\", df['ZN'].median())\n",
    "df['ZN'].hist(bins=20)\n",
    "# most zn values are 0, so we can impute with the median 0. non zero values are sparse and scattered\n",
    "df['ZN'].fillna(df['ZN'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb1293-c798-49c6-af91-f4e7736ffad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"imputing LSTAT\n",
    "While LSTAT (percentage of lower-status population) is less erratic than AGE or ZN, it's still:\n",
    "Right-skewed (long tail on the high end),\n",
    "Not smoothly varying enough to trust interpolation,\n",
    "And lacks strong, consistent correlation with other features (as shown in your scatter plots).\n",
    "\"\"\"\n",
    "print(\"Mean LSTAT:\", df['LSTAT'].mean())\n",
    "print(\"Median LSTAT:\", df['LSTAT'].median())\n",
    "df['LSTAT'].hist(bins=20)\n",
    "df['LSTAT'].fillna(df['LSTAT'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e92089-fc2c-451a-9840-db693b2ad94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing for INDUS\n",
    "# first we plot INDUS values against row index to see if any pattern stands out\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df.index, df['INDUS'], linestyle='-', marker='o', markersize=3, color='steelblue')\n",
    "plt.title(\"INDUS Values Across Row Index\")\n",
    "plt.xlabel(\"Row Index\")\n",
    "plt.ylabel(\"INDUS (Proportion of non-retail business acres)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ead6ae-a57d-4c76-91c4-3ae0de1b70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# IMPUTATION STRATEGY FOR 'INDUS' FEATURE\n",
    "# ---------------------------------------\n",
    "\n",
    "# When we plotted INDUS against row index, we observed long plateaus â€” e.g., stretches where \n",
    "# many rows had the same INDUS value like 18.1 or 19.58, followed by sudden changes. \n",
    "# This gave us the intuition that these plateaus might not be random â€” maybe theyâ€™re the result \n",
    "# of natural groupings in the data (like similar towns or zones), and those groupings might also \n",
    "# be reflected in other features like TAX, NOX, AGE, or DIS.\n",
    "\n",
    "# That led us to consider using KNN or clustering for imputation:\n",
    "# If rows with similar INDUS values also tend to have similar values in other features,\n",
    "# then we could find \"neighborhoods\" in feature space and use them to impute missing INDUS values.\n",
    "\n",
    "# However, this is just a visual hypothesis based on the index â€” which may have no geographic or \n",
    "# neighborhood meaning. The plateaus might be real, or INDUS could be largely independent. \n",
    "# We cannot assume KNN or clustering will work unless we test this properly.\n",
    "\n",
    "# To justify KNN:\n",
    "# - We would first check individual correlations:\n",
    "#     df.corr()['INDUS'].sort_values(ascending=False)\n",
    "#   and select features with moderate correlation (|r| > 0.3).\n",
    "# - Even if no individual feature is highly correlated, combinations of features (e.g., TAX + NOX + AGE)\n",
    "#   might form meaningful neighborhoods â€” which KNN could use.\n",
    "# - To explore this, we would need dimensionality reduction tools like PCA, t-SNE, or UMAP to check \n",
    "#   if similar INDUS values cluster in reduced feature space.\n",
    "# - We could also validate clustering structure using silhouette scores or visual inspection.\n",
    "\n",
    "# Practical limitations with KNN or clustering:\n",
    "# - We must exclude the target variable MEDV to prevent data leakage during imputation.\n",
    "# - We must also exclude CRIM since it has missing values â€” KNN requires all features to be non-null.\n",
    "# - Not all features are guaranteed to help predict INDUS â€” including irrelevant ones may add noise.\n",
    "# - Scaling is required since KNN is distance-based.\n",
    "# - Both KNN and clustering require tuning, validation, and extra computation.\n",
    "\n",
    "# Clustering (e.g., KMeans) is another option â€” group similar rows and impute INDUS with the groupâ€™s mean â€”\n",
    "# but it comes with the same overhead: handling missing values, feature selection, preprocessing, and validation.\n",
    "\n",
    "# Given all this effort â€” and the fact that INDUS has only 20 missing values â€” we choose a simple, safe\n",
    "# approach for now:\n",
    "#     - Use **mode** (most frequent value) for imputation â€” since INDUS has dominant repeated values.\n",
    "#     - Alternatively, use **median** if distribution appears continuous but skewed.\n",
    "\n",
    "# Later, once the model is trained, we can use SHAP values or other interpretability tools to evaluate \n",
    "# how important INDUS actually was. If it turns out to be significant, we can revisit its imputation \n",
    "# and apply a more rigorous method like KNN or clustering-based approaches.\n",
    "\n",
    "# FINAL IMPUTATION (for now):\n",
    "df['INDUS'].fillna(df['INDUS'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecc54f-03b7-4cc1-86f6-5badab9c9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing CRIM\n",
    "# Overlay missing points in red Xs on plot of CRIM values against row indices.\n",
    "missing_indices = df[df['CRIM'].isna()].index\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df.index, df['CRIM'], linestyle='-', marker='o', markersize=3, color='darkred', label='CRIM')\n",
    "plt.scatter(missing_indices, [0]*len(missing_indices), color='black', marker='x', label='Missing')\n",
    "plt.title(\"CRIM Values Across Row Index (Missing Highlighted)\")\n",
    "plt.xlabel(\"Row Index\")\n",
    "plt.ylabel(\"CRIM\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400f72a-a176-4abb-a2ed-2a0392828e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CRIM is mostly very low, clustered near 0, for the majority of rows.\n",
    "Around row 360â€“460, thereâ€™s a sharp rise with many spikes up to 80â€“90.\n",
    "The missing values are scattered throughout, including in both the low-CRIM and high-CRIM zones.\n",
    "So there are clearly two major regimes:\n",
    "- Low-crime zones (dense, flat region near 0)\n",
    "- High-crime zones (spiky cluster later in index)\n",
    "\n",
    "Possible Approachs:\n",
    "1.  Clustering-Based Imputation\n",
    "    Cluster rows into 2â€“3 groups using all complete features, and impute CRIM using the average (or median) of the cluster the row belongs to.\n",
    "    Why it makes sense:\n",
    "    The plot clearly suggests CRIM behaves in clusters.\n",
    "    If neighborhoods with high TAX/NOX/AGE correspond to high CRIM, clustering will capture that.\n",
    "2.  Classification-Based Imputation\n",
    "    Treat imputation as a supervised task: train a model to predict CRIM range (e.g., low vs high crime), and use that to infer missing values.\n",
    "    Feels overkill, especially for only 20 missing values â€” and not ideal unless you're imputing categories, not continuous values.\n",
    "3.  Index-Based Averaging (Local Neighborhoods)\n",
    "    Use a simple moving window (Â±2 or Â±3 rows) to take the average of nearby non-null values.\n",
    "    This is quick and may work well for the low-CRIM areas, where values are stable.\n",
    "    But:\n",
    "    Around high-crime regions (spiky zone), this may be too unstable\n",
    "    Still ignores feature similarity, only uses position\n",
    "    So it's fine as a \"quick fix\", but not ideal for robust modeling. \n",
    "4.  Mode / Median Imputation\n",
    "    This would ignore the obvious structure and plateau regions â€” so while it's quick and safe, youâ€™d lose too much information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58731df5-f78f-47e0-b0c1-6b56452ee7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select features (exclude CRIM and MEDV)\n",
    "features_for_clustering = ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', \n",
    "                           'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']  # All complete now\n",
    "\n",
    "# 2. Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[features_for_clustering])\n",
    "\n",
    "# 3. Fit KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df['CRIM_cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Create a list to store comparison info for previously missing CRIM rows\n",
    "crim_comparison = []\n",
    "\n",
    "# 4. Impute CRIM and track comparison with window mean\n",
    "for cluster_label in df['CRIM_cluster'].unique():\n",
    "    # Get indices of missing CRIM values in this cluster BEFORE filling\n",
    "    missing_indices = df[\n",
    "        (df['CRIM_cluster'] == cluster_label) & (df['CRIM'].isna())\n",
    "    ].index\n",
    "\n",
    "    # Compute cluster median from CRIM values that are not missing\n",
    "    cluster_median = df.loc[\n",
    "        (df['CRIM_cluster'] == cluster_label) & (df['CRIM'].notna()), 'CRIM'\n",
    "    ].median()\n",
    "\n",
    "    for idx in missing_indices:\n",
    "        # Impute using cluster median\n",
    "        df.at[idx, 'CRIM'] = cluster_median\n",
    "\n",
    "        # Compute rolling window mean (excluding current row)\n",
    "        window_range = range(max(0, idx - 2), min(len(df), idx + 3))\n",
    "        window_vals = df.loc[list(window_range), 'CRIM']\n",
    "        window_vals = window_vals.drop(idx)\n",
    "\n",
    "        if not window_vals.empty:\n",
    "            window_mean = window_vals.mean()\n",
    "            diff = abs(cluster_median - window_mean)\n",
    "\n",
    "            crim_comparison.append({\n",
    "                'Index': idx,\n",
    "                'CRIM_cluster_imputed': cluster_median,\n",
    "                'CRIM_window_mean': window_mean,\n",
    "                'Difference': diff\n",
    "            })\n",
    "\n",
    "# 5. Convert comparison results to DataFrame\n",
    "crim_comparison_df = pd.DataFrame(crim_comparison)\n",
    "\n",
    "# 6. Visualize clusters in 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['CRIM_cluster'], cmap='Accent', alpha=0.7)\n",
    "plt.title(\"KMeans Clusters in 2D PCA Space (excluding CRIM and MEDV)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc83275-7ce4-4c23-a308-0e2135e393a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted cluster-wise median imputation for CRIM based on feature similarity using KMeans clustering.\n",
    "# As a lightweight secondary validation, we compared the imputed values against a rolling window mean \n",
    "# (Â±2 rows) based on row index. While the window mean is simpler and purely local, it tends to fluctuate \n",
    "# more, especially in high-CRIM regions with sharp spikes. The comparison confirmed that cluster median \n",
    "# imputations are generally more stable and better aligned with feature-based groupings.\n",
    "crim_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bb1d0-131f-4d75-a8ad-ea6050b07ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"âš ï¸ Note: The code blocks below for imputing CRIM assume that CRIM is the only column with missing values.\n",
    "If other features in the dataframe also contain missing values, clustering-based or KNN-based imputation for CRIM will fail,\n",
    "since these methods rely on other features (which must be complete) to compute similarity or distance.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Plot CRIM values with wherein CRIM value is the y-axis and row index is the x-axis\n",
    "\"\"\"\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df.index, df['CRIM'], marker='o', linestyle='-', label='CRIM')\n",
    "plt.xlabel('Row Index')\n",
    "plt.ylabel('CRIM Value')\n",
    "plt.title('CRIM values across dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Simple imputation (mean, median) or linear interpolation won't work here because CRIM is highly skewed:\n",
    "Most values are near 0, but there's a sharp spike around index 370â€“420.\n",
    "Mean/median would bias estimates too low, and interpolation would miss abrupt regional jumps.\n",
    "Imputation must account for local context â€” clustering or KNN is better suited for this distribution.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "We intentionally exclude the 'CRIM' column when performing clustering for imputation.\n",
    "\n",
    "Including 'CRIM' in clustering would lead to two key issues:\n",
    "1. You can't compute distances for rows where 'CRIM' is missing â€” so you can't assign those rows to clusters.\n",
    "2. Even worse, if 'CRIM' is a major factor driving the separation between clusters (e.g., some clusters have high crime rates, others low), \n",
    "   using it during clustering introduces bias. You're using the very value you're trying to impute to define the group it belongs to â€” \n",
    "   which is a form of data leakage and circular logic.\n",
    "\n",
    "Instead, we use only contextual features (like RM, LSTAT, TAX, etc.) to build clusters. This allows us to impute missing 'CRIM' values \n",
    "based on the typical behavior of similar neighborhoods, inferred through available variables.\n",
    "\n",
    "This approach is not perfect, but it's valid â€” because we're using indirect signals to estimate 'CRIM', rather than CRIM itself.\n",
    "\n",
    "To further improve reliability, we layer two imputation methods:\n",
    "1. K-Means Clustering:\n",
    "   - Groups samples based on contextual patterns.\n",
    "   - Imputes missing 'CRIM' using cluster-level statistics (e.g., mean or median).\n",
    "   - Helps understand the global structure â€” such as which clusters represent high-CRIM vs low-CRIM areas.\n",
    "\n",
    "2. KNN Imputation:\n",
    "   - Looks at each sample individually.\n",
    "   - Uses the most similar neighboring rows (based on other features) to estimate missing 'CRIM'.\n",
    "   - Acts as a secondary verification or refinement of the cluster-based estimate.\n",
    "\n",
    "In short, we use clustering to build broader context, and KNN to refine the local estimate. \n",
    "Straight KNN alone can sometimes be sensitive to noise and outliers, especially in skewed datasets, so combining both gives more robust imputation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Define feature set (exclude CRIM)\n",
    "# ---------------------------\n",
    "features = ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n",
    "            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "# Split into complete and missing sets based on CRIM\n",
    "df_complete = df[df['CRIM'].notna()].copy()\n",
    "df_missing = df[df['CRIM'].isna()].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: KMeans Clustering on df_complete\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_complete_scaled = scaler.fit_transform(df_complete[features])\n",
    "\n",
    "# You can tune n_clusters based on silhouette scores\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df_complete['cluster'] = kmeans.fit_predict(X_complete_scaled)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Assign clusters to missing rows (based on same features)\n",
    "# ---------------------------\n",
    "X_missing_scaled = scaler.transform(df_missing[features])\n",
    "missing_clusters = kmeans.predict(X_missing_scaled)\n",
    "\n",
    "# Get average CRIM per cluster\n",
    "cluster_crim_map = df_complete.groupby('cluster')['CRIM'].mean()\n",
    "\n",
    "# Impute CRIM in missing rows using cluster averages\n",
    "df_kmeans_imputed = df.copy()\n",
    "df_kmeans_imputed.loc[df['CRIM'].isna(), 'CRIM'] = [\n",
    "    cluster_crim_map[cluster] for cluster in missing_clusters\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: KNN Imputation\n",
    "# ---------------------------\n",
    "# Scale all features except CRIM\n",
    "scaler_knn = StandardScaler()\n",
    "scaled_features = scaler_knn.fit_transform(df[features])\n",
    "\n",
    "# Combine scaled features + CRIM for KNN\n",
    "df_knn_input = pd.DataFrame(scaled_features, columns=features)\n",
    "df_knn_input['CRIM'] = df['CRIM'].values\n",
    "\n",
    "# Apply KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed_array = knn_imputer.fit_transform(df_knn_input)\n",
    "\n",
    "# Extract only CRIM column after imputation\n",
    "crim_knn_imputed = df_knn_imputed_array[:, df_knn_input.columns.get_loc('CRIM')]\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Compare KMeans and KNN imputations\n",
    "# ---------------------------\n",
    "missing_indices = df[df['CRIM'].isna()].index\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'KMeans_CRIM': df_kmeans_imputed.loc[missing_indices, 'CRIM'].values,\n",
    "    'KNN_CRIM': crim_knn_imputed[missing_indices]\n",
    "})\n",
    "comparison_df['Diff'] = np.abs(comparison_df['KMeans_CRIM'] - comparison_df['KNN_CRIM'])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"ðŸ“Š Imputation Comparison Summary:\")\n",
    "print(comparison_df.describe())\n",
    "\n",
    "# Optional plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(comparison_df['KMeans_CRIM'].values, label='KMeans CRIM', marker='o')\n",
    "plt.plot(comparison_df['KNN_CRIM'].values, label='KNN CRIM', marker='x')\n",
    "plt.title(\"Comparison of Imputed CRIM values (Missing Rows)\")\n",
    "plt.xlabel(\"Index within missing rows\")\n",
    "plt.ylabel(\"Imputed CRIM\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e817e8-4ebd-4b25-8852-883a88a1340a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431c07f-b622-4403-b716-f185888b8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used features ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'] for clustering\n",
    "# Since KMeans requires all inputs to be numeric floats, we don't have to check dtype of these columns.\n",
    "# Since, we have not explored the target columns MEDV yet, we can run some checks on it.\n",
    "df['MEDV'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ceb2e4-ec73-4fd2-9215-a8e6715a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features (excluding target)\n",
    "features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n",
    "            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "# Generate and save plots\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df[feature], df['MEDV'], alpha=0.5)\n",
    "    plt.title(f'{feature} vs MEDV')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('MEDV')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()  # Close to prevent overlapping plots in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31b11b-cedb-48e4-941d-d3daa2a73074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# WHY WE PLOT FEATURES AGAINST TARGET USING ORIGINAL VALUES\n",
    "# -------------------------------------------------------------\n",
    "# We do *not* plot features after standard scaling. Instead, we use the original,\n",
    "# unscaled (but imputed) dataset to visualize how each feature relates to the target (MEDV).\n",
    "#\n",
    "# Here's why:\n",
    "# ðŸ“Š Interpretability: We want to understand the real-world relationship\n",
    "#     between features like RM (number of rooms) or TAX (property tax) and MEDV.\n",
    "# ðŸ” Meaningful axes: After scaling, features lose their original units and are centered \n",
    "#     around 0 with std ~1. The plot becomes harder to interpret.\n",
    "# ðŸš« Target isn't scaled: Since MEDV remains in original units (e.g., $1000s), plotting \n",
    "#     it against scaled features would be visually misleading.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# FEATURE ENGINEERING DECISIONS BASED ON SCATTER PLOTS\n",
    "# -------------------------------------------------------------\n",
    "# By plotting each feature vs. MEDV, we identified patterns that suggest non-linear \n",
    "# relationships. To help linear regression capture these, we applied transformations:\n",
    "#     - df['LOG_CRIM']  = np.log1p(df['CRIM'])     # CRIM is skewed â†’ log transform\n",
    "#     - df['LSTAT_SQ']  = df['LSTAT'] ** 2         # LSTAT has a curved relation â†’ square\n",
    "#     - df['RM_SQ']     = df['RM'] ** 2            # RM may show non-linearity â†’ square\n",
    "#\n",
    "# Other features showed weak or noisy relationships with MEDV, so we kept them as-is.\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# WHY SOME FEATURES (e.g., ZN, CHAS, RAD) ARE NOT TRANSFORMED\n",
    "# -------------------------------------------------------------\n",
    "# Some features like 'ZN' (proportion of residential land zoned), 'CHAS' (Charles River dummy),\n",
    "# and 'RAD' (index of accessibility to radial highways) showed **vertical lines** in their\n",
    "# scatter plots with MEDV. This means:\n",
    "#   - These features take on only a **few discrete values**\n",
    "#   - For each value, MEDV varies across a wide range\n",
    "#\n",
    "# âœ³ï¸ These features are **categorical or ordinal in nature** â€” transforming them using\n",
    "# log or squaring would be meaningless and possibly harmful.\n",
    "#\n",
    "# ðŸ‘‰ It's better to **keep them as-is**, since:\n",
    "#   - They're already encoded meaningfully (e.g., 0/1 for CHAS)\n",
    "#   - Their interaction with the target might be captured through model coefficients\n",
    "#   - Attempting to \"linearize\" them doesn't help, because the feature's informativeness\n",
    "#     comes from **category differences**, not continuous scale\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# WHEN AND WHY TO DO FEATURE ENGINEERING\n",
    "# -------------------------------------------------------------\n",
    "# âž¤ Feature engineering is especially important for **linear regression** models, \n",
    "#    which can only model straight-line (linear) relationships between features and target.\n",
    "#\n",
    "# âž¤ By transforming features, we try to make **nonlinear relationships look linear**, \n",
    "#    allowing the linear model to perform better.\n",
    "#\n",
    "# âž¤ In more complex models like **SVMs (with kernels)**, **decision trees**, or **XGBoost**, \n",
    "#    this step is often unnecessary â€” those models can naturally handle non-linear patterns.\n",
    "#\n",
    "# âž¤ So for linear regression, feature engineering becomes a powerful and necessary tool \n",
    "#    to build a robust, predictive model.\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PCA: WHEN TO USE IT AND WHY IT HELPS\n",
    "# -------------------------------------------------------------\n",
    "# PCA (Principal Component Analysis) is a dimensionality reduction technique.\n",
    "# It can be helpful when:\n",
    "#     - You have many correlated features (collinearity)\n",
    "#     - You want to reduce feature count and avoid overfitting\n",
    "#     - You don't need interpretability for individual features\n",
    "#\n",
    "# ðŸ”„ PCA works by combining correlated features into **principal components** â€”\n",
    "#     new features that are orthogonal (i.e., uncorrelated with each other).\n",
    "#     This removes redundancy and helps address **multicollinearity**, which can\n",
    "#     be a major problem in linear models (e.g., unstable coefficients, high variance).\n",
    "#\n",
    "# ðŸ“‰ Instead of dropping features manually, PCA lets us project the data into\n",
    "#     a smaller number of components that still explain most of the variance.\n",
    "#     This often results in simpler models that generalize better.\n",
    "#\n",
    "# In our case, we havenâ€™t used PCA yet â€” but it could be useful later if we\n",
    "# decide to:\n",
    "#     - Reduce dimensionality\n",
    "#     - Denoise the dataset\n",
    "#     - Prepare the data for clustering, SVMs, or regularized linear models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3fcbe-bbf8-4849-81f7-3a0072cfb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LOG_CRIM'] = np.log1p(df['CRIM']) \n",
    "df['LSTAT_SQ'] = df['LSTAT'] ** 2\n",
    "df['RM_SQ'] = df['RM'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544f048-6f2a-4c11-b91f-c557d5c6211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs of (original, transformed) features\n",
    "feature_pairs = [\n",
    "    ('CRIM', 'LOG_CRIM'),\n",
    "    ('LSTAT', 'LSTAT_SQ'),\n",
    "    ('RM', 'RM_SQ')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "for i, (raw_feat, eng_feat) in enumerate(feature_pairs):\n",
    "    # Left: original feature vs MEDV\n",
    "    plt.subplot(3, 2, 2*i + 1)\n",
    "    plt.scatter(df[raw_feat], df['MEDV'], alpha=0.6, s=20, color='gray')\n",
    "    plt.xlabel(raw_feat)\n",
    "    plt.ylabel('MEDV')\n",
    "    plt.title(f'{raw_feat} vs MEDV')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Right: engineered feature vs MEDV\n",
    "    plt.subplot(3, 2, 2*i + 2)\n",
    "    plt.scatter(df[eng_feat], df['MEDV'], alpha=0.6, s=20, color='teal')\n",
    "    plt.xlabel(eng_feat)\n",
    "    plt.ylabel('MEDV')\n",
    "    plt.title(f'{eng_feat} vs MEDV')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b60118-2954-47f1-810e-f461015eb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Did feature engineering transformations work, ie linearise and smoothen relationship curve between x and target?\n",
    "1. LSTAT vs MEDV shows a nice downward slope, somewhat linear-ish.\n",
    "   LSTAT_SQ vs MEDV squashes low LSTAT values together and exaggerates high values, making the relationship more curved and less interpretable.\n",
    "   Verdict: Don't Keep\n",
    "2. The original plot (CRIM vs MEDV) is highly skewed and concentrated near zero with some huge outliers (e.g., >80).\n",
    "   After log1p, the spread is compressed, and the downward trend with MEDV becomes visibly smoother.\n",
    "   Verdict: Keep\n",
    "3. RM vs MEDV is already pretty linear and clean.\n",
    "   RM_SQ vs MEDV stretches values unnecessarily â€” and makes the plot look more quadratic, not more linear.\n",
    "   Verdict: Don't Keep\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d65f7-10a9-426a-9ee2-cc0846cb39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new transformed features\n",
    "df['INV_LSTAT'] = 1 / df['LSTAT'] \n",
    "df['LOG_LSTAT'] = np.log1p(df['LSTAT'])   \n",
    "df['LOG_RM'] = np.log(df['RM'])\n",
    "df['EXP_INV_RM'] = np.exp(-df['RM']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe53146-c192-4408-9e41-d467b8920189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# --- Row 1: LSTAT comparisons ---\n",
    "lstat_feats = ['LSTAT', 'LOG_LSTAT', 'INV_LSTAT']\n",
    "for i, feature in enumerate(lstat_feats):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.scatter(df[feature], df['MEDV'], alpha=0.6, s=20, color='purple')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('MEDV')\n",
    "    plt.title(f'{feature} vs MEDV')\n",
    "    plt.grid(True)\n",
    "\n",
    "# --- Row 2: RM comparisons ---\n",
    "rm_feats = ['RM', 'LOG_RM', 'EXP_INV_RM']\n",
    "for i, feature in enumerate(rm_feats):\n",
    "    plt.subplot(2, 3, i + 4)\n",
    "    plt.scatter(df[feature], df['MEDV'], alpha=0.6, s=20, color='darkgreen')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('MEDV')\n",
    "    plt.title(f'{feature} vs MEDV')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e3261-e745-4843-9655-f5e2ddef578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these curves again are worse or compress higher values, while only being slightly better. This would lead to loss in interpretability. Hence, we will not feature enigneer columns LSTAT and RM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fea33-190b-4dda-81c2-802f86ba479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dabdba-4c8e-4d66-bb59-7b173f65b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['LSTAT_SQ', 'RM_SQ', 'INV_LSTAT', 'LOG_LSTAT', 'LOG_RM', 'EXP_INV_RM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192ca22-99ae-490e-9aa4-9b3865808ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='CRIM', inplace=True)\n",
    "# Since we will be using LOG_CRIM, we should drop CRIM feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead77a6-d650-4c95-b097-19dc57d1d271",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8a152-2727-4c18-8a4f-2adec71e5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# SHOULD WE SCALE THE TARGET VARIABLE ('MEDV')?\n",
    "# -----------------------------------------------\n",
    "\n",
    "# The short answer: it depends on the type of model you're using.\n",
    "\n",
    "# WHY WE MIGHT WANT TO SCALE THE TARGET:\n",
    "# Scaling (e.g., StandardScaler or MinMaxScaler) transforms the target variable \n",
    "# to a smaller, more consistent range â€” which can help models that rely on \n",
    "# gradient-based optimization (like neural networks) train more effectively.\n",
    "\n",
    "# But not all models are sensitive to the scale of the target. Here's the breakdown:\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. TREE-BASED MODELS (e.g., RandomForest, XGBoost)\n",
    "# -------------------------------------------------------\n",
    "# Do NOT scale the target variable.\n",
    "# These models split data based on thresholds, not distances or gradients.\n",
    "# Changing the scale of the target won't affect how the trees split or make predictions.\n",
    "# Keeping the target in its original units (e.g., thousands of dollars) also helps interpret results.\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. LINEAR MODELS (e.g., Linear Regression, Ridge, Lasso)\n",
    "# -----------------------------------------------------------\n",
    "# Scaling the target is OPTIONAL.\n",
    "# - If your model is solving the normal equation (analytically), scaling isn't needed.\n",
    "# - If you're using gradient descent (e.g., in PyTorch/TF), scaling might speed up convergence.\n",
    "# - The downside is: coefficients lose their original units, so interpretability decreases.\n",
    "\n",
    "# In most practical cases, it's fine to leave the target unscaled, especially \n",
    "# when using models like `sklearn.linear_model.LinearRegression`.\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. NEURAL NETWORKS / DEEP LEARNING MODELS\n",
    "# --------------------------------------------------------\n",
    "# DO scale the target variable.\n",
    "# These models benefit from both input and output being in a similar scale/range,\n",
    "# such as [0, 1] or having zero mean and unit variance.\n",
    "# Without scaling, the model may converge slowly or get stuck due to gradient instability.\n",
    "# You'll need to inverse-transform the predictions after training to get them \n",
    "# back into real-world units.\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# TL;DR:\n",
    "# -----------------------------------------------------\n",
    "# - Tree models â†’ DON'T scale target\n",
    "# - Linear models â†’ Optional (depends on solver + convergence)\n",
    "# - Neural networks â†’ DO scale target (and inverse-transform predictions later)\n",
    "\n",
    "# For this project (Boston Housing dataset), if you're using linear regression,\n",
    "# it's fine to keep 'MEDV' in its original units (house price in $1000s) â€” \n",
    "# which is also more interpretable when evaluating predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96909511-5648-451d-8129-e845040da7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# FEATURE SCALING (StandardScaler)\n",
    "# -------------------------------------------------------------\n",
    "# We will apply standard scaling (zero mean, unit variance) to the input features,\n",
    "# but we will NOT scale the target variable 'MEDV', since we're using linear regression\n",
    "# and want to keep output predictions in interpretable dollar units ($1000s).\n",
    "\n",
    "# It's important to perform train-test split BEFORE scaling,\n",
    "# because scaling depends on the mean and standard deviation of the data.\n",
    "# If we scale before splitting, we would leak information from the test set\n",
    "# into the training process, which would give an unrealistic performance estimate.\n",
    "\n",
    "# Note: We'll also scale the 'CHAS' column, even though it's binary (0/1),\n",
    "# because StandardScaler will just center it to have zero mean.\n",
    "# Tree-based models typically donâ€™t require scaling for binary features,\n",
    "# but for linear models, it's consistent and harmless to scale all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685060f-efdf-4366-a1cb-77058f2d436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and target\n",
    "features = ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "       'PTRATIO', 'B', 'LSTAT', 'LOG_CRIM']\n",
    "target = 'MEDV'\n",
    "\n",
    "# Split dataset into features and target\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split into train and test sets (before any scaling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply StandardScaler to input features only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6df609-9361-4600-9736-5186483a51b7",
   "metadata": {},
   "source": [
    "# Do we need PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885347cd-b8f4-4db0-9c1c-433fdb93c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only have 13 features in the Boston Housing dataset, and we're using a simple Linear Regression model.\n",
    "# Unless there's strong multicollinearity (high correlation between features), we don't need PCA.\n",
    "# PCA is typically used when:\n",
    "#   - We have a large number of features\n",
    "#   - Features are highly correlated\n",
    "#   - We want to reduce dimensionality or noise\n",
    "#\n",
    "# In logistic regression, PCA can be useful because orthogonal (uncorrelated) features improve the stability \n",
    "# and speed of convergence of the optimization algorithm used to maximize the log-likelihood.\n",
    "# Orthogonal features also help regularization (L1/L2) work more effectively:\n",
    "#   - If two features are correlated, a regularized model may struggle to penalize both correctly.\n",
    "#   - With uncorrelated features, it's easier for the model to shrink irrelevant ones toward zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c57cb-6b3c-44eb-b02a-41225c09b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Understanding Variance Inflation Factor (VIF)\n",
    "# ----------------------------------------------\n",
    "# VIF is a tool used to detect multicollinearity in regression problems.\n",
    "# Multicollinearity happens when two or more features (columns) in your dataset are highly correlated â€”\n",
    "# meaning they contain overlapping or redundant information.\n",
    "\n",
    "# Why is that a problem?\n",
    "# Imagine you're trying to predict house prices using both \"number of rooms\" and \"total floor area\".\n",
    "# If every house with more rooms also tends to have more floor area, then both features are telling\n",
    "# the model almost the same thing. The model gets \"confused\" about how much weight to give each one.\n",
    "\n",
    "# VIF helps you identify such situations.\n",
    "# For each feature, VIF measures **how much it can be predicted using the other features**.\n",
    "# If a feature has a high VIF, it means it is almost a combination of other features â€”\n",
    "# and may not be useful on its own.\n",
    "\n",
    "# Real-life analogy:\n",
    "# Think of asking two friends for directions â€” and both give you nearly identical answers.\n",
    "# Asking both didn't give you new information â€” in fact, it might just make you second-guess\n",
    "# or overcomplicate your choice. VIF flags that kind of redundancy.\n",
    "\n",
    "# Rule of thumb:\n",
    "# - VIF = 1 â†’ No multicollinearity (ideal)\n",
    "# - VIF > 5 â†’ Some concern (you may want to look closer)\n",
    "# - VIF > 10 â†’ Serious multicollinearity (consider dropping or combining features)\n",
    "\n",
    "# You can think of VIF as a more advanced version of Pearson correlation:\n",
    "# While Pearson checks the relationship between two variables at a time,\n",
    "# VIF checks whether a single feature can be predicted from all the others combined.\n",
    "# It's a more complete measure for identifying redundancy before fitting regression models.\n",
    "\n",
    "# In this step, we'll compute the VIF for each feature in the scaled training set to check\n",
    "# whether any are highly correlated and may need to be removed or transformed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4f7c6-3eb7-4269-be84-249a05a3538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(X, feature_names):\n",
    "    vif_dict = {}\n",
    "    for i in range(X.shape[1]):\n",
    "        X_i = X[:, i]\n",
    "        X_other = np.delete(X, i, axis=1)\n",
    "\n",
    "        # Regress feature i on all others\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_other, X_i)\n",
    "        r_squared = model.score(X_other, X_i)\n",
    "\n",
    "        vif = 1 / (1 - r_squared) if r_squared < 1 else np.inf\n",
    "        vif_dict[feature_names[i]] = vif\n",
    "\n",
    "    return pd.DataFrame({'Feature': list(vif_dict.keys()), 'VIF': list(vif_dict.values())})\n",
    "\n",
    "# computing VIF on scaled data\n",
    "vif_df = compute_vif(X_train_scaled, features)\n",
    "print(vif_df.sort_values(by='VIF', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf0d6d-03a8-4ad9-9c59-08708b2431d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAD and TAX are known to be highly correlated in the Boston Housing dataset â€” both relate to infrastructure and zoning.\\\n",
    "\"\"\"\n",
    "Approaches to resolve collinearity:\n",
    "1. Drop one of RAD or TAX.\n",
    "2. Keep both but use ridge/lasso regularisation to reduce overfitting, while keeping the features.\n",
    "3. Apply PCA, if we prefer prefer transforming correlated features into orthogonal ones, but we will lose feature interpretability.\n",
    "\"\"\"\n",
    "\n",
    "# We start with dropping RAD from the unscaled feature sets\n",
    "X_train_pruned = X_train.drop(columns=['RAD'])\n",
    "X_test_pruned = X_test.drop(columns=['RAD'])\n",
    "\n",
    "# Even though dropping a column like 'RAD' does not change the mean or standard deviation\n",
    "# of the remaining columns (like 'TAX'), we still need to re-fit the StandardScaler.\n",
    "# This is because the scaler internally stores the order and number of columns during fit.\n",
    "# If we drop a column, the positions of other columns shift, and reusing the old scaler\n",
    "# could apply the wrong scaling to the wrong column.\n",
    "# So even though the math behind scaling each column stays the same,\n",
    "# we redo StandardScaler to make sure it lines up correctly with the pruned dataframe.\n",
    "# REDOING SCALING ON FEATURE SETS\n",
    "\n",
    "X_train_pruned_scaled = scaler.fit_transform(X_train_pruned)\n",
    "\n",
    "# Recalculate VIF on re-scaled feature sets to see if it improves\n",
    "vif_pruned_df = compute_vif(X_train_pruned_scaled, X_train_pruned.columns)\n",
    "print(vif_pruned_df.sort_values(by='VIF', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0433e5-7ed5-4ba7-b1f2-88aec1e1f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced multicollinearity without using PCA\n",
    "# collinearity has reduced to <5, which can still be monitored, but is acceptable for traning a model\n",
    "\n",
    "# Scaling pruned test data using the same scalar\n",
    "# feature scaling the new input training and testing datasets \n",
    "X_test_pruned_scaled = scaler.transform(X_test_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6726954-b7c8-4075-96d3-1b216b837530",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pruned_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faac15d-742d-48b1-9b29-35132b243c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_from_scratch(X, y, learning_rate=0.01, num_epochs = 1000, min_step = 1e-6, verbose = True):\n",
    "    \"\"\"\n",
    "    Implementing Linear Regression from Scratch with detailed equation steps.\n",
    "    Parameters: \n",
    "    X: array of shape(num_samples, num_features) containing training data\n",
    "    y: array of shape(num_samples, ) containing target values\n",
    "    learning_rate: float value which is learning rate of gradient descent\n",
    "    num_epochs: int, maximum number of epochs\n",
    "    min_step: float, minimum step size for convergence beyond which we stop training\n",
    "    verbose: bool, whether to print progress\n",
    "\n",
    "    Returns: \n",
    "    weights: array, shape (num_features) Coefficients for the features\n",
    "    bias: float, intercept term\n",
    "    loss_history: list, lost history during training\n",
    "    \"\"\"\n",
    "    m, n = X.shape # getting number of samples and features\n",
    "\n",
    "    # print the initial equation\n",
    "    if verbose:\n",
    "        print(\"Linear Regression Equation:\")\n",
    "        eq = \"y = \"\n",
    "        for j in range(n):\n",
    "            eq += f\"w{j+1}*x{j+1} + \"\n",
    "        eq += \"b\"\n",
    "        print(eq)\n",
    "        \n",
    "        print(\"\\nMean Squared Error (MSE): \")\n",
    "        print(\"MSE = (1/m) * Î£(y_i - (wâ‚*xâ‚_i + wâ‚‚*xâ‚‚_i + ... + wâ‚™*xâ‚™_i + b))Â²\")\n",
    "        print(\"\\nGradient for each weight w_j:\")\n",
    "        print(\"âˆ‚MSE/âˆ‚w_j = (2/m) * Î£((wâ‚*xâ‚_i + wâ‚‚*xâ‚‚_i + ... + wâ‚™*xâ‚™_i + b - y_i) * x_j_i)\")\n",
    "        \n",
    "        print(\"\\nGradient for bias b:\")\n",
    "        print(\"âˆ‚MSE/âˆ‚b = (2/m) * Î£(wâ‚*xâ‚_i + wâ‚‚*xâ‚‚_i + ... + wâ‚™*xâ‚™_i + b - y_i)\")\n",
    "        \n",
    "        print(\"\\nUpdate Rules:\")\n",
    "        print(\"w_j = w_j - learning_rate * âˆ‚MSE/âˆ‚w_j\")\n",
    "        print(\"b = b - learning_rate * âˆ‚MSE/âˆ‚b\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    #Initialise weights and biases\n",
    "    # we start with random weights and zero bias\n",
    "    weights = np.random.randn(n)\n",
    "    bias = 0.0\n",
    "    loss_history = []\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(num_epochs):\n",
    "        # Step 1: Forward Pass â€“ make predictions using current weight and bias\n",
    "        # Å· = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "\n",
    "        # Step 2: Calculate the error (difference between predicted and actual target)\n",
    "        error = y_pred - y  # (Å· - y)\n",
    "\n",
    "        # Step 3: Calculate Mean Squared Error\n",
    "        # MSE = (1/m) * Î£(y_i - Å·_i)Â²\n",
    "        loss = np.mean(error**2)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Step 4: Calculate gradients\n",
    "        # For weights: âˆ‚MSE/âˆ‚w_j = (2/m) * Î£((Å·_i - y_i) * x_j_i)\n",
    "        gradient_weights = (2/m) * np.dot(X.T, error)\n",
    "\n",
    "        # For bias: âˆ‚MSE/âˆ‚b = (2/m) * Î£(Å·_i - y_i)\n",
    "        gradient_bias = (2/m) * np.sum(error)\n",
    "\n",
    "        # Step 5: Update Weights and Bias\n",
    "        # w_j = w_j - learning_rate * âˆ‚MSE/âˆ‚w_j\n",
    "        weight_step = learning_rate * gradient_weights\n",
    "        weights = weights - weight_step\n",
    "\n",
    "        # b = b - learning_rate * âˆ‚MSE/âˆ‚b\n",
    "        bias_step = learning_rate * gradient_bias\n",
    "        bias = bias - bias_step\n",
    "\n",
    "        # checks if ALL weight steps are smaller than our minimum step threshold and bias_step is smaller than minimum step\n",
    "        # Check for convergence\n",
    "        if np.all(np.abs(weight_step) < min_step) and np.abs(bias_step) < min_step:\n",
    "            if verbose:\n",
    "                print(f\"\\nConverged at Epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # print progress at every 100 epochs if verbose is set to True\n",
    "        if verbose and epoch%100 == 0:\n",
    "            print(f\"\\nEpoch {epoch}, Loss: {loss:.6f}\")\n",
    "            \n",
    "            # show the current equation (only at major milestones)\n",
    "            if epoch%500 == 0:\n",
    "                eq = f\"y = \"\n",
    "                for j in range(n):\n",
    "                    eq += f\"{weights[j]:.4f}*x{j+1} + \"\n",
    "                eq += f\"{bias:.4f}\"\n",
    "                print(f\"\\nCurrent Equation: {eq}\")\n",
    "\n",
    "    # Final Equation:\n",
    "    if verbose:\n",
    "        print(\"\\nFinal Linear Regression Equation:\")\n",
    "        eq = f\"y = \"\n",
    "        for j in range(n):\n",
    "            eq += f\"{weights[j]:.4f}*x{j+1} + \"\n",
    "        eq += f\"{bias:.4f}\"\n",
    "        print(eq)\n",
    "    \n",
    "    return weights, bias, loss_history\n",
    "                    \n",
    "# --------------------------------------------------------------\n",
    "# Why do we need a forward pass in gradient descent?\n",
    "# --------------------------------------------------------------\n",
    "# The short answer: because of the **chain rule**.\n",
    "# \n",
    "# We're trying to minimize the Mean Squared Error (MSE):\n",
    "#     MSE = (1/n) * Î£ (y_i - Å·_i)Â²\n",
    "# where:\n",
    "#     Å·_i = w Â· x_i + b   â† prediction for each sample i\n",
    "#\n",
    "# To update weights using gradient descent, we need:\n",
    "#     âˆ‚MSE / âˆ‚w_j\n",
    "# \n",
    "# But MSE is a function of predictions (Å·), not directly of weights.\n",
    "# So we must apply the **chain rule**:\n",
    "# \n",
    "#     âˆ‚MSE / âˆ‚w_j = âˆ‚MSE / âˆ‚Å·_i Ã— âˆ‚Å·_i / âˆ‚w_j\n",
    "# \n",
    "# Let's break that down:\n",
    "#\n",
    "# 1. Derivative of MSE w.r.t. prediction:\n",
    "#       âˆ‚MSE / âˆ‚Å·_i = 2 * (Å·_i - y_i)\n",
    "# \n",
    "# 2. Derivative of prediction w.r.t. weight w_j:\n",
    "#       âˆ‚Å·_i / âˆ‚w_j = x_ij  â† the j-th feature of sample i\n",
    "#\n",
    "# So to compute the full gradient:\n",
    "#     âˆ‚MSE / âˆ‚w_j = (2/n) * Î£ [ (Å·_i - y_i) * x_ij ]\n",
    "#\n",
    "# Therefore, we **must compute the predictions (Å·_i)** before we can:\n",
    "#     - Compute the error (Å·_i - y_i)\n",
    "#     - Use the chain rule to get gradients\n",
    "#\n",
    "# Conclusion:\n",
    "# The forward pass is required not to \"monitor progress\", \n",
    "# but because it's **mathematically required** by the **chain rule** \n",
    "# to compute how MSE changes with respect to weights.\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b4fba-2f75-4f84-958f-8a7343c3df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    # Make predictions using Learned weight and bias\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def evaluate_model(X_test, y_test, weights, bias):\n",
    "    # Calculate various metrics to evaluate the model on test data\n",
    "    # Make predictions on test data\n",
    "    y_pred = predict(X_test, weights, bias)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse = np.mean((y_test - y_pred)**2)\n",
    "\n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    \n",
    "    # RÂ² Score (coefficient of determination)\n",
    "    ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "    ss_residual = np.sum((y_test - y_pred)**2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'RÂ²': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402e4fe-1cbe-43ef-a814-72f78d923e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values for test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : array-like\n",
    "        True target values for test data\n",
    "    y_pred : array-like\n",
    "        Predicted values for test data\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Plot the perfect prediction line\n",
    "    min_val = min(np.min(y_test), np.min(y_pred))\n",
    "    max_val = max(np.max(y_test), np.max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.title('Test Data: Actual vs Predicted Values')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b895f8-8c99-42cc-8e15-a89523c0f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(loss_history):\n",
    "    \"\"\"Plot the loss history during training\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Loss History (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79d830-235e-4386-a160-b895c244f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data is X_train_pruned_scaled, X_test_pruned_scaled, y_train, y_test\n",
    "\n",
    "# training the model\n",
    "weights, bias, loss_history = linear_regression_from_scratch(X_train_pruned_scaled, y_train, learning_rate=0.01, \n",
    "                                                             num_epochs=1000, min_step=1e-6, verbose = True)\n",
    "\n",
    "# Evaluating the model\n",
    "metrics = evaluate_model(X_test_pruned_scaled, y_test, weights, bias)\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot the loss history\n",
    "plot_loss_history(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb4d16-5bf0-4ba1-a43f-64727d14100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "y_pred = predict(X_test_pruned_scaled, weights, bias)\n",
    "\n",
    "# Plot actual vs predicted values on test data\n",
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d43400-8d7a-4e91-85ff-a8882253297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning Rate Effects:\n",
    "\n",
    "- Linear Models: Affects convergence speed but not final model (if converged)\n",
    "  â€¢ Loss surface is convex (bowl-shaped) with a single global minimum\n",
    "  â€¢ Too large: May oscillate or diverge\n",
    "  â€¢ Too small: Converges very slowly\n",
    "  â€¢ Typical values: 0.001-0.1 (start with 0.01)\n",
    "\n",
    "- Non-Linear Models: Affects both convergence AND final model quality\n",
    "  â€¢ Different rates can lead to different local minima\n",
    "\n",
    "- Feature Scaling: Critical for consistent gradient steps\n",
    "  â€¢ Unscaled features â†’ large steps in some dimensions â†’ potential overshooting\n",
    "  â€¢ Always scale features before applying gradient descent\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dc856-54c0-40a5-9ebb-2d628fcd96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with a learning rate of 0.1\n",
    "\n",
    "# training the model\n",
    "weights, bias, loss_history = linear_regression_from_scratch(X_train_pruned_scaled, y_train, learning_rate=0.1, \n",
    "                                                             num_epochs=1000, min_step=1e-6, verbose = True)\n",
    "\n",
    "# Evaluating the model\n",
    "metrics = evaluate_model(X_test_pruned_scaled, y_test, weights, bias)\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot the loss history\n",
    "plot_loss_history(loss_history)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = predict(X_test_pruned_scaled, weights, bias)\n",
    "\n",
    "# Plot actual vs predicted values on test data\n",
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaec61-9ad3-44a8-881b-28aef06f4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very minimal difference, overall model is kinda the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f729a-553c-469b-9c11-fed1a90029ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
